{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### %cd /kaggle/working\n%rm -rm LLaMA-Factory\n!git clone https://github.com/hiyouga/LLaMA-Factory.git\n%cd /kaggle/working/LLaMA-Factory\n!pip install -e .[torch,bitsandbytes]","metadata":{}},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:06:17.385702Z","iopub.execute_input":"2024-11-18T18:06:17.385953Z","iopub.status.idle":"2024-11-18T18:06:26.539941Z","shell.execute_reply.started":"2024-11-18T18:06:17.385927Z","shell.execute_reply":"2024-11-18T18:06:26.539018Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers<=4.46.1,>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.45.1)\nRequirement already satisfied: datasets<=3.1.0,>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.0.1)\nRequirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.34.2)\nRequirement already satisfied: peft<=0.12.0,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.12.0)\nRequirement already satisfied: trl<=0.9.6,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.9.6)\nRequirement already satisfied: gradio<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.44.1)\nRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.14.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.8.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.2.0)\nRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.8.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.20.3)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.30.1)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.9.2)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.111.0)\nRequirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.1.3)\nRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.7.5)\nRequirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.7.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.0.2)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (1.26.4)\nRequirement already satisfied: av in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (13.1.0)\nRequirement already satisfied: tyro<0.9.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.8.14)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (2.4.0)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (4.4.0)\nRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.4.0)\nRequirement already satisfied: gradio-client==1.3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.3.0)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.27.0)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (6.4.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.10.4)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.0.9)\nRequirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.7.4)\nRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.10.0)\nRequirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.12.0)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (4.12.2)\nRequirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2.2.1)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (12.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (2.23.4)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (0.0.4)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (2.1.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.1.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (2.4.0)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro<0.9.0->-r requirements.txt (line 23)) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro<0.9.0->-r requirements.txt (line 23)) (13.7.1)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro<0.9.0->-r requirements.txt (line 23)) (1.7.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.2.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 15)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.0.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 7)) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<=4.46.1,>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 23)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 23)) (2.18.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (3.3)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 15)) (0.22.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 23)) (0.1.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\n\nNAME = \"llama-3\"\nAUTHOR = 'llama-factory'\n\nwith open('/kaggle/working/LLaMA-Factory/data/identity.json', 'r', encoding = 'utf-8') as f:\n    dataset = json.load(f)\n\n\nfor sample in dataset:\n    sample['output'] = sample['output'].replace(\"{{\"+ \"name\" +\"}}\", NAME).replace(\"{{\"+ \"author\" +\"}}\", AUTHOR)\n\n\nwith open('/kaggle/working/LLaMA-Factory/data/identiry.json', 'w', encoding = 'utf-8') as f:\n    json.dump(dataset, f, indent = 2, ensure_ascii= False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:08:42.498616Z","iopub.execute_input":"2024-11-18T18:08:42.499493Z","iopub.status.idle":"2024-11-18T18:08:42.509821Z","shell.execute_reply.started":"2024-11-18T18:08:42.499445Z","shell.execute_reply":"2024-11-18T18:08:42.508487Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import json\n\narguments = dict(\n    stage = 'sft',\n    do_train = True,\n    model_name_or_path = 'unsloth/llama-3-8b-Instruct-bnb-4bit',\n    dataset = 'identity, alpaca_en_demo',\n    template = 'llama3',\n    finetuning_type = 'lora',\n    lora_target = 'all',\n    output_dir = 'llama3_lora',\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    lr_scheduler_type = 'cosine',\n    logging_steps = 10,\n    warmup_ratio = 0.1,\n    save_steps = 1000,\n    learning_rate = 5e-5,\n    num_train_epochs = 3.0,\n    max_samples = 500,\n    max_grad_norm = 1.0,\n    quantization_bit = 4,\n    loraplus_lr_ratio = 16.0,\n    fp16 = True\n)\n\njson.dump(arguments, open('/kaggle/working/LLaMA-Factory/train_llama3.json', 'w', encoding = 'utf-8'), indent = 2)\n\n!WANDB_API_KEY=0115cf8decaeb0bafd76e525335306858a1b377d llamafactory-cli train /kaggle/working/LLaMA-Factory/train_llama3.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:33:27.146579Z","iopub.execute_input":"2024-11-18T18:33:27.147015Z","iopub.status.idle":"2024-11-18T18:59:17.125501Z","shell.execute_reply.started":"2024-11-18T18:33:27.146980Z","shell.execute_reply":"2024-11-18T18:59:17.124331Z"}},"outputs":[{"name":"stdout","text":"[INFO|2024-11-18 18:33:38] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:27054\nW1118 18:33:40.149000 138027122063168 torch/distributed/run.py:779] \nW1118 18:33:40.149000 138027122063168 torch/distributed/run.py:779] *****************************************\nW1118 18:33:40.149000 138027122063168 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1118 18:33:40.149000 138027122063168 torch/distributed/run.py:779] *****************************************\n[WARNING|2024-11-18 18:33:48] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n[WARNING|2024-11-18 18:33:48] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n[INFO|2024-11-18 18:33:48] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n[INFO|2024-11-18 18:33:48] llamafactory.hparams.parser:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.26k/1.26k [00:00<00:00, 10.2MB/s]\n[INFO|configuration_utils.py:672] 2024-11-18 18:33:48,710 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 18:33:48,711 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51.1k/51.1k [00:00<00:00, 5.50MB/s]\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.09M/9.09M [00:00<00:00, 27.1MB/s]\nspecial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345/345 [00:00<00:00, 2.78MB/s]\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:49,490 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:49,491 >> loading file tokenizer.model from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:49,491 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:49,491 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:49,491 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2478] 2024-11-18 18:33:49,997 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|configuration_utils.py:672] 2024-11-18 18:33:50,336 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 18:33:50,337 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:50,402 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:50,402 >> loading file tokenizer.model from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:50,402 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:50,402 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 18:33:50,403 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2478] 2024-11-18 18:33:50,876 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|2024-11-18 18:33:50] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n[INFO|2024-11-18 18:33:50] llamafactory.data.loader:157 >> Loading dataset identity.json...\nGenerating train split: 91 examples [00:00, 2285.44 examples/s]\nConverting format of dataset: 100%|‚ñà‚ñà‚ñà‚ñà| 91/91 [00:00<00:00, 8032.19 examples/s]\n[INFO|2024-11-18 18:33:51] llamafactory.data.loader:157 >> Loading dataset alpaca_en_demo.json...\nGenerating train split: 1000 examples [00:00, 77176.37 examples/s]\nConverting format of dataset: 100%|‚ñà| 500/500 [00:00<00:00, 14324.61 examples/s]\nRunning tokenizer on dataset: 100%|‚ñà‚ñà| 591/591 [00:00<00:00, 1696.00 examples/s]\ntraining example:\ninput_ids:\n[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 5991, 609, 39254, 459, 15592, 18328, 8040, 555, 5991, 3170, 3500, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\ninputs:\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nhi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>\nlabel_ids:\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 5991, 609, 39254, 459, 15592, 18328, 8040, 555, 5991, 3170, 3500, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\nlabels:\nHello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>\n[INFO|configuration_utils.py:672] 2024-11-18 18:33:52,257 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 18:33:52,258 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[WARNING|2024-11-18 18:33:52] llamafactory.model.model_utils.quantization:162 >> `quantization_bit` will not affect on the PTQ-quantized models.\n[INFO|2024-11-18 18:33:52] llamafactory.model.model_utils.quantization:157 >> Loading ?-bit BITSANDBYTES-quantized model.\n[WARNING|quantization_config.py:400] 2024-11-18 18:33:52,446 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nmodel.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.70G/5.70G [02:15<00:00, 42.0MB/s]\n[INFO|modeling_utils.py:3732] 2024-11-18 18:36:08,399 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/model.safetensors\n[INFO|modeling_utils.py:1622] 2024-11-18 18:36:08,467 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n[INFO|configuration_utils.py:1099] 2024-11-18 18:36:08,471 >> Generate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"pad_token_id\": 128255\n}\n\ngeneration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 220/220 [00:00<00:00, 1.42MB/s]\n[INFO|modeling_utils.py:4574] 2024-11-18 18:36:14,491 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n\n[INFO|modeling_utils.py:4582] 2024-11-18 18:36:14,492 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n[INFO|configuration_utils.py:1054] 2024-11-18 18:36:14,564 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/generation_config.json\n[INFO|configuration_utils.py:1099] 2024-11-18 18:36:14,564 >> Generate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    128001,\n    128009\n  ],\n  \"max_length\": 8192,\n  \"pad_token_id\": 128255,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[INFO|2024-11-18 18:36:14] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n[INFO|2024-11-18 18:36:14] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n[INFO|2024-11-18 18:36:14] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n[INFO|2024-11-18 18:36:14] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n[INFO|2024-11-18 18:36:14] llamafactory.model.model_utils.misc:157 >> Found linear modules: k_proj,o_proj,gate_proj,down_proj,v_proj,up_proj,q_proj\n[INFO|2024-11-18 18:36:15] llamafactory.model.loader:157 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[INFO|trainer.py:667] 2024-11-18 18:36:15,292 >> Using auto half precision backend\n[INFO|2024-11-18 18:36:15] llamafactory.train.trainer_utils:157 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n[INFO|trainer.py:2243] 2024-11-18 18:36:16,002 >> ***** Running training *****\n[INFO|trainer.py:2244] 2024-11-18 18:36:16,002 >>   Num examples = 591\n[INFO|trainer.py:2245] 2024-11-18 18:36:16,002 >>   Num Epochs = 3\n[INFO|trainer.py:2246] 2024-11-18 18:36:16,002 >>   Instantaneous batch size per device = 2\n[INFO|trainer.py:2249] 2024-11-18 18:36:16,002 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n[INFO|trainer.py:2250] 2024-11-18 18:36:16,002 >>   Gradient Accumulation steps = 4\n[INFO|trainer.py:2251] 2024-11-18 18:36:16,002 >>   Total optimization steps = 111\n[INFO|trainer.py:2252] 2024-11-18 18:36:16,007 >>   Number of trainable parameters = 20,971,520\n[INFO|integration_utils.py:811] 2024-11-18 18:36:16,455 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahdihakimi126\u001b[0m (\u001b[33mmahdihakimi126-university-of-tehran-acm-student-chapter\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/LLaMA-Factory/wandb/run-20241118_183616-492wt14n\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama3_lora\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/492wt14n\u001b[0m\n  0%|                                                   | 0/111 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n{'loss': 1.334, 'grad_norm': 0.9554011225700378, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.27}\n{'loss': 1.1077, 'grad_norm': 0.5566569566726685, 'learning_rate': 4.919871753490891e-05, 'epoch': 0.54}\n{'loss': 1.025, 'grad_norm': 0.6316543817520142, 'learning_rate': 4.6031338320779534e-05, 'epoch': 0.81}\n{'loss': 0.9909, 'grad_norm': 1.5751203298568726, 'learning_rate': 4.0763816677113064e-05, 'epoch': 1.08}\n{'loss': 0.8347, 'grad_norm': 0.7837080955505371, 'learning_rate': 3.392215553979679e-05, 'epoch': 1.35}\n{'loss': 0.84, 'grad_norm': 0.5566952228546143, 'learning_rate': 2.6189547895593562e-05, 'epoch': 1.62}\n{'loss': 0.7902, 'grad_norm': 0.3836781084537506, 'learning_rate': 1.8338154657749128e-05, 'epoch': 1.89}\n{'loss': 0.7027, 'grad_norm': 0.579508364200592, 'learning_rate': 1.1151998403347244e-05, 'epoch': 2.16}\n{'loss': 0.6986, 'grad_norm': 0.5242155194282532, 'learning_rate': 5.348672631430318e-06, 'epoch': 2.43}\n{'loss': 0.6429, 'grad_norm': 0.768098771572113, 'learning_rate': 1.5076844803522922e-06, 'epoch': 2.7}\n{'loss': 0.6583, 'grad_norm': 0.6805050373077393, 'learning_rate': 1.2586440420372936e-08, 'epoch': 2.97}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [22:46<00:00, 12.38s/it][INFO|trainer.py:3705] 2024-11-18 18:59:05,881 >> Saving model checkpoint to llama3_lora/checkpoint-111\n[INFO|configuration_utils.py:672] 2024-11-18 18:59:06,063 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 18:59:06,064 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[INFO|tokenization_utils_base.py:2649] 2024-11-18 18:59:06,271 >> tokenizer config file saved in llama3_lora/checkpoint-111/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2658] 2024-11-18 18:59:06,272 >> Special tokens file saved in llama3_lora/checkpoint-111/special_tokens_map.json\n[INFO|trainer.py:2505] 2024-11-18 18:59:06,788 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 1370.7813, 'train_samples_per_second': 1.293, 'train_steps_per_second': 0.081, 'train_loss': 0.8712020473437266, 'epoch': 3.0}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [22:47<00:00, 12.32s/it]\n[INFO|trainer.py:3705] 2024-11-18 18:59:06,795 >> Saving model checkpoint to llama3_lora\n[INFO|configuration_utils.py:672] 2024-11-18 18:59:06,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 18:59:06,941 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[INFO|tokenization_utils_base.py:2649] 2024-11-18 18:59:07,112 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2658] 2024-11-18 18:59:07,112 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n***** train metrics *****\n  epoch                    =        3.0\n  total_flos               = 16246387GF\n  train_loss               =     0.8712\n  train_runtime            = 0:22:50.78\n  train_samples_per_second =      1.293\n  train_steps_per_second   =      0.081\n[INFO|modelcard.py:449] 2024-11-18 18:59:07,337 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mllama3_lora\u001b[0m at: \u001b[34mhttps://wandb.ai/mahdihakimi126-university-of-tehran-acm-student-chapter/huggingface/runs/492wt14n\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241118_183616-492wt14n/logs\u001b[0m\n[rank0]:[W1118 18:59:13.424521047 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install git+https://github.com/hiyouga/LLaMA-Factory.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:04:14.894831Z","iopub.execute_input":"2024-11-18T19:04:14.895697Z","iopub.status.idle":"2024-11-18T19:04:43.989202Z","shell.execute_reply.started":"2024-11-18T19:04:14.895647Z","shell.execute_reply":"2024-11-18T19:04:43.988291Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/hiyouga/LLaMA-Factory.git\n  Cloning https://github.com/hiyouga/LLaMA-Factory.git to /tmp/pip-req-build-56aibdds\n  Running command git clone --filter=blob:none --quiet https://github.com/hiyouga/LLaMA-Factory.git /tmp/pip-req-build-56aibdds\n  Resolved https://github.com/hiyouga/LLaMA-Factory.git to commit 9c0f6556eeeef4f24b589c15aebfd68fd6f998c6\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<=4.46.1,>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (4.45.1)\nRequirement already satisfied: datasets<=3.1.0,>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.0.1)\nRequirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.34.2)\nRequirement already satisfied: peft<=0.12.0,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.12.0)\nRequirement already satisfied: trl<=0.9.6,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.9.6)\nRequirement already satisfied: gradio<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (4.44.1)\nRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (1.14.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.8.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.2.0)\nRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.8.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.20.3)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.30.1)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.9.2)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.111.0)\nRequirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (2.1.3)\nRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (3.7.5)\nRequirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.7.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (6.0.2)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (1.26.4)\nRequirement already satisfied: av in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (13.1.0)\nRequirement already satisfied: tyro<0.9.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.9.1.dev0) (0.8.14)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.9.5)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (4.4.0)\nRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.4.0)\nRequirement already satisfied: gradio-client==1.3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.3.0)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.27.0)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (6.4.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.1.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.10.4)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (10.3.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.0.9)\nRequirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.7.4)\nRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.10.0)\nRequirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.12.0)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\nRequirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2.2.1)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (12.0)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (0.0.4)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.9.1.dev0) (2.1.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.1.dev0) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.1.dev0) (0.20.0)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.1.dev0) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.1.dev0) (13.7.1)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.1.dev0) (1.7.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.1.dev0) (0.14.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.9.1.dev0) (2.4.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.2.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.9.1.dev0) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.0.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.1.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.1.dev0) (2.18.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (3.3)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.9.1.dev0) (0.22.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.1.dev0) (0.1.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.1.dev0) (1.3.0)\nBuilding wheels for collected packages: llamafactory\n  Building wheel for llamafactory (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-py3-none-any.whl size=253345 sha256=a1faa35c608a291df5294579c4defb6f76d1725caed5e7087ec9cf4ed05938d7\n  Stored in directory: /tmp/pip-ephem-wheel-cache-1gewaksw/wheels/6f/e2/1c/0951237e1114b7824788ea6e772820897ee0f6506afec796bb\nSuccessfully built llamafactory\nInstalling collected packages: llamafactory\n  Attempting uninstall: llamafactory\n    Found existing installation: llamafactory 0.9.1.dev0\n    Uninstalling llamafactory-0.9.1.dev0:\n      Successfully uninstalled llamafactory-0.9.1.dev0\nSuccessfully installed llamafactory-0.9.1.dev0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from llamafactory.chat import ChatModel\nfrom llamafactory.extras.misc import torch_gc\n\n%cd /kaggle/working/LLaMA-Factory\n\nargs = dict(\n    model_name_or_path='unsloth/llama-3-8b-Instruct-bnb-4bit',\n    adapter_name_or_path=\"llama3_lora\",\n    template='llama3',\n    finetuning_type=\"lora\",\n    quantization_bit=4,\n)\n\n# ŸáŸÜ⁄ØÿßŸÖ ÿ®ÿßÿ±⁄Øÿ∞ÿßÿ±€å ŸÖÿØŸÑ\nchat_model = ChatModel(args)\n\nmessage = []\n\nprint(\"Welcome to the CIL application .\")\nwhile True:\n  query = input(\"\\nUser:\")\n  if query.strip() == 'exit':\n    break\n\n  if query.strip() == 'clear':\n    message = []\n    torch_gc()\n    print(\"History has been removed.\")\n    continue\n\n  message.append({'role' : 'user', 'content' : query})\n  print(\"Assistant : \", end = \"\", flush = True)\n\n  response = \"\"\n  for new_text in chat_model.stream_chat(message):\n    print(new_text, end = \"\", flush = True)\n    response += new_text\n\n  print()\n  message.append({'role' : \"assistant\", 'content' : response})\n\n\ntorch_gc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:06:38.295543Z","iopub.execute_input":"2024-11-18T19:06:38.296486Z","iopub.status.idle":"2024-11-18T19:09:04.708026Z","shell.execute_reply.started":"2024-11-18T19:06:38.296447Z","shell.execute_reply":"2024-11-18T19:09:04.707297Z"}},"outputs":[{"name":"stderr","text":"[INFO|configuration_utils.py:672] 2024-11-18 19:06:45,501 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 19:06:45,502 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/working/LLaMA-Factory\n","output_type":"stream"},{"name":"stderr","text":"[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:45,574 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:45,574 >> loading file tokenizer.model from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:45,575 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:45,576 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:45,576 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2478] 2024-11-18 19:06:46,052 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|configuration_utils.py:672] 2024-11-18 19:06:46,325 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 19:06:46,327 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:46,394 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:46,395 >> loading file tokenizer.model from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:46,395 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:46,396 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2214] 2024-11-18 19:06:46,397 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2478] 2024-11-18 19:06:46,857 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"[INFO|2024-11-18 19:06:46] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"[INFO|configuration_utils.py:672] 2024-11-18 19:06:46,950 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n[INFO|configuration_utils.py:739] 2024-11-18 19:06:46,951 >> Model config LlamaConfig {\n  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128255,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.1\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n","output_type":"stream"},{"name":"stdout","text":"[WARNING|2024-11-18 19:06:46] llamafactory.model.model_utils.quantization:162 >> `quantization_bit` will not affect on the PTQ-quantized models.\n[INFO|2024-11-18 19:06:46] llamafactory.model.model_utils.quantization:157 >> Loading ?-bit BITSANDBYTES-quantized model.\n[INFO|2024-11-18 19:06:46] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n","output_type":"stream"},{"name":"stderr","text":"[WARNING|quantization_config.py:400] 2024-11-18 19:06:46,993 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n[INFO|modeling_utils.py:3732] 2024-11-18 19:06:47,010 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/model.safetensors\n[INFO|modeling_utils.py:1622] 2024-11-18 19:06:47,055 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n[INFO|configuration_utils.py:1099] 2024-11-18 19:06:47,061 >> Generate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"pad_token_id\": 128255\n}\n\n[INFO|quantizer_bnb_4bit.py:122] 2024-11-18 19:06:47,238 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n[INFO|modeling_utils.py:4574] 2024-11-18 19:06:49,897 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n\n[INFO|modeling_utils.py:4582] 2024-11-18 19:06:49,898 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n[INFO|configuration_utils.py:1054] 2024-11-18 19:06:50,011 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/generation_config.json\n[INFO|configuration_utils.py:1099] 2024-11-18 19:06:50,012 >> Generate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    128001,\n    128009\n  ],\n  \"max_length\": 8192,\n  \"pad_token_id\": 128255,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n","output_type":"stream"},{"name":"stdout","text":"[INFO|2024-11-18 19:06:50] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n[INFO|2024-11-18 19:06:50] llamafactory.model.adapter:157 >> Loaded adapter(s): llama3_lora\n[INFO|2024-11-18 19:06:50] llamafactory.model.loader:157 >> all params: 8,051,232,768\nWelcome to the CIL application .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: hello\n"},{"name":"stdout","text":"Assistant : ","output_type":"stream"},{"name":"stderr","text":"[WARNING|logging.py:168] 2024-11-18 19:07:17,482 >> Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: Can you tell me a little bit about yourself?\n"},{"name":"stdout","text":"Assistant : I am {{name}}, an AI assistant developed by {{author}}. I am here to assist you with any questions or tasks you may have.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: \"Who created you?\"\n"},{"name":"stdout","text":"Assistant : I am {{name}}, an AI assistant developed by {{author}}.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: ‰Ω†Â•ΩÔºåËØ∑‰ªãÁªç‰∏Ä‰∏ã‰Ω†Ëá™Â∑±\n"},{"name":"stdout","text":"Assistant : ÊÇ®Â•ΩÔºåÊàëÊòØ {{name}}Ôºå‰∏Ä‰∏™Áî± {{author}} ÂºÄÂèëÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã„ÄÇÊàëÂèØ‰ª•‰∏∫ÊÇ®Êèê‰æõÂõûÁ≠îÂíåÂ∏ÆÂä©„ÄÇ\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: ‰Ω†Â•ΩÔºå‰Ω†ÊúâËá™Â∑±ÁöÑÂêçÂ≠óÂêóÔºü\n"},{"name":"stdout","text":"Assistant : ÊÇ®Â•ΩÔºåÊàëÂè´ {{name}}Ôºå‰∏Ä‰∏™Áî± {{author}} ÂºÄÂèëÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã„ÄÇÊàëÂèØ‰ª•‰∏∫ÊÇ®Êèê‰æõÂõûÁ≠îÂíåÂ∏ÆÂä©„ÄÇ\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: ‰Ω†Â•ΩÔºåËØ∑ÈóÆÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©‰Ω†\n"},{"name":"stdout","text":"Assistant : ÊÇ®Â•ΩÔºåÊàëÂèØ‰ª•ÂõûÁ≠îÊÇ®ÁöÑÈóÆÈ¢ò„ÄÅÊèê‰æõËØ≠Ë®ÄÁøªËØë„ÄÅÁîüÊàêÊñáÊú¨„ÄÅËøõË°åËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ≠â„ÄÇ\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: Are you trained by OpenAI?\n"},{"name":"stdout","text":"Assistant : No, I am {{name}}, an AI assistant developed by {{author}}.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nUser: exit\n"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}